#!/usr/bin/env python3
"""
RAGçŸ¥è¯†åˆ‡ç‰‡åæ ‡å®šä½å™¨ - ä¼˜åŒ–ç‰ˆ
ä¸“é—¨ç”¨äºåœ¨PDFä¸­å®šä½RAGç³»ç»Ÿçš„çŸ¥è¯†åˆ‡ç‰‡ä½ç½®ï¼Œè¿”å›æœ€ä½³åŒ¹é…åŒºåŸŸ
"""

import re
from typing import List, Dict, Any, Tuple, Optional
from difflib import SequenceMatcher
import json

def find_rag_chunk_coordinates(chunk_text: str, pdf_path: str, 
                              similarity_threshold: float = 0.7,
                              return_best_only: bool = True) -> List[Dict[str, Any]]:
    """
    åœ¨PDFä¸­å®šä½RAGçŸ¥è¯†åˆ‡ç‰‡çš„åæ ‡ - è¿”å›æœ€ä½³åŒ¹é…åŒºåŸŸ
    
    Args:
        chunk_text (str): RAGçŸ¥è¯†åˆ‡ç‰‡çš„æ–‡æœ¬å†…å®¹
        pdf_path (str): PDFæ–‡ä»¶è·¯å¾„
        similarity_threshold (float): æ–‡æœ¬ç›¸ä¼¼åº¦é˜ˆå€¼ (0-1)
        return_best_only (bool): æ˜¯å¦åªè¿”å›æœ€ä½³åŒ¹é…
    
    Returns:
        list: åŒ…å«åˆ‡ç‰‡ä½ç½®ä¿¡æ¯çš„åˆ—è¡¨ï¼ˆé»˜è®¤åªè¿”å›æœ€ä½³åŒ¹é…ï¼‰
    """
    try:
        import pymupdf as fitz
    except ImportError:
        try:
            import fitz
        except ImportError:
            raise ImportError("è¯·å®‰è£…PyMuPDF: pip install pymupdf")
    
    # é¢„å¤„ç†åˆ‡ç‰‡æ–‡æœ¬
    chunk_text_clean = _clean_text(chunk_text)
    chunk_length = len(chunk_text_clean)
    
    print(f"æ­£åœ¨åˆ†æåˆ‡ç‰‡... é•¿åº¦: {chunk_length} å­—ç¬¦")
    
    # æ ¹æ®æ–‡æœ¬é•¿åº¦é€‰æ‹©ä¸åŒçš„åŒ¹é…ç­–ç•¥
    if chunk_length < 100:
        print("ğŸ“ ä½¿ç”¨çŸ­æ–‡æœ¬åŒ¹é…ç­–ç•¥")
        return _find_short_text_coordinates(chunk_text_clean, pdf_path, similarity_threshold, return_best_only)
    else:
        print("ğŸ“– ä½¿ç”¨é•¿æ–‡æœ¬åŒ¹é…ç­–ç•¥")
        return _find_long_text_coordinates(chunk_text_clean, pdf_path, similarity_threshold, return_best_only)


def _find_short_text_coordinates(chunk_text: str, pdf_path: str, 
                                similarity_threshold: float, return_best_only: bool) -> List[Dict[str, Any]]:
    """çŸ­æ–‡æœ¬åŒ¹é…ç­–ç•¥ - æ›´çµæ´»çš„åŒ¹é…æ–¹æ³•"""
    
    try:
        import pymupdf as fitz
    except ImportError:
        import fitz
    
    # çŸ­æ–‡æœ¬ä½¿ç”¨æ›´ç®€å•çš„å…³é”®è¯
    chunk_keywords = _extract_short_text_keywords(chunk_text)
    print(f"çŸ­æ–‡æœ¬å…³é”®è¯: {chunk_keywords[:3]}")
    
    results = []
    doc = fitz.open(pdf_path)
    
    for page_num in range(doc.page_count):
        page = doc[page_num]
        
        # æ–¹æ³•1: ç›´æ¥æ–‡æœ¬æœç´¢
        direct_matches = _find_direct_text_matches(page, chunk_text, page_num)
        results.extend(direct_matches)
        
        # æ–¹æ³•2: å…³é”®è¯åŒ¹é…ï¼ˆé™ä½é˜ˆå€¼ï¼‰
        keyword_matches = _find_keyword_matches(page, chunk_keywords, chunk_text, page_num, similarity_threshold * 0.6)
        results.extend(keyword_matches)
        
        # æ–¹æ³•3: æ¨¡ç³ŠåŒ¹é…ï¼ˆè¿›ä¸€æ­¥é™ä½é˜ˆå€¼ï¼‰
        fuzzy_matches = _find_fuzzy_matches(page, chunk_text, page_num, similarity_threshold * 0.4)
        results.extend(fuzzy_matches)
    
    doc.close()
    
    # çŸ­æ–‡æœ¬ç»“æœå¤„ç†
    results = _process_short_text_results(results, return_best_only)
    
    print(f"çŸ­æ–‡æœ¬åŒ¹é…æ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
    return results


def _find_long_text_coordinates(chunk_text: str, pdf_path: str, 
                               similarity_threshold: float, return_best_only: bool) -> List[Dict[str, Any]]:
    """é•¿æ–‡æœ¬åŒ¹é…ç­–ç•¥ - åŸæœ‰çš„æ•´ä½“åŒ¹é…æ–¹æ³•"""
    
    try:
        import pymupdf as fitz
    except ImportError:
        import fitz
    
    chunk_keywords = _extract_key_phrases(chunk_text)
    print(f"é•¿æ–‡æœ¬å…³é”®è¯: {chunk_keywords[:5]}")
    
    results = []
    doc = fitz.open(pdf_path)
    
    # éå†æ¯ä¸€é¡µï¼Œå¯»æ‰¾æœ€ä½³åŒ¹é…åŒºåŸŸ
    for page_num in range(doc.page_count):
        page = doc[page_num]
        
        # æ–¹æ³•1: æ•´é¡µæ–‡æœ¬åŒ¹é…
        page_text = page.get_text()
        page_text_clean = _clean_text(page_text)
        
        # è®¡ç®—æ•´ä½“ç›¸ä¼¼åº¦
        overall_similarity = _calculate_similarity(chunk_text, page_text_clean)
        
        if overall_similarity >= similarity_threshold:
            # æ‰¾åˆ°é«˜ç›¸ä¼¼åº¦é¡µé¢ï¼Œå®šä½å…·ä½“åŒºåŸŸ
            best_region = _locate_best_region_in_page(
                page, chunk_text, chunk_keywords, page_num
            )
            if best_region:
                results.append(best_region)
        
        # æ–¹æ³•2: å…³é”®è¯å¯†åº¦åŒ¹é…
        elif _calculate_keyword_density(chunk_keywords, page_text_clean) > 0.3:
            # å…³é”®è¯å¯†åº¦é«˜ï¼Œå¯èƒ½æ˜¯éƒ¨åˆ†åŒ¹é…
            region = _locate_keyword_region(
                page, chunk_keywords, page_num, chunk_text
            )
            if region and region['similarity'] >= similarity_threshold * 0.8:
                results.append(region)
    
    doc.close()
    
    # æŒ‰ç›¸ä¼¼åº¦æ’åº
    results.sort(key=lambda x: x['similarity'], reverse=True)
    
    # å»é‡ç›¸è¿‘çš„åŒºåŸŸ
    results = _remove_duplicate_regions(results)
    
    print(f"é•¿æ–‡æœ¬åŒ¹é…æ‰¾åˆ° {len(results)} ä¸ªå€™é€‰åŒºåŸŸ")
    
    # æ ¹æ®å‚æ•°å†³å®šè¿”å›ç»“æœ
    if return_best_only and results:
        print(f"è¿”å›æœ€ä½³åŒ¹é…: ç›¸ä¼¼åº¦ {results[0]['similarity']:.3f}")
        return [results[0]]
    
    return results


def _extract_short_text_keywords(text: str) -> List[str]:
    """ä¸ºçŸ­æ–‡æœ¬æå–å…³é”®è¯ - æ›´ç®€å•çš„ç­–ç•¥"""
    # çŸ­æ–‡æœ¬ç›´æ¥ä½¿ç”¨æœ‰æ„ä¹‰çš„è¯æ±‡
    words = re.findall(r'[\u4e00-\u9fff]{2,}|[a-zA-Z]{3,}', text)
    
    # è¿‡æ»¤å¸¸è§åœç”¨è¯
    stopwords = {'çš„', 'å’Œ', 'æˆ–', 'åŠ', 'ä¸', 'å¯¹', 'ä»', 'åœ¨', 'ä¸º', 'æ˜¯', 'äº†', 'åˆ°', 'ç”±', 'æœ‰', 'è¢«', 'æ‰€', 'ç­‰', 'è¿™', 'é‚£', 'ä¸€ä¸ª', 'å¯ä»¥', 'åº”è¯¥'}
    keywords = [word for word in words if word not in stopwords and len(word) >= 2]
    
    # çŸ­æ–‡æœ¬ä¿ç•™æ›´å¤šå…³é”®è¯
    return keywords[:10]


def _find_direct_text_matches(page, chunk_text: str, page_num: int) -> List[Dict[str, Any]]:
    """ç›´æ¥æ–‡æœ¬æœç´¢åŒ¹é…"""
    matches = []
    import pymupdf as fitz
    # å°è¯•æ‰¾åˆ°åŒ…å«éƒ¨åˆ†æ–‡æœ¬çš„åŒºåŸŸ
    text_instances = page.search_for(chunk_text[:20])  # æœç´¢å‰20ä¸ªå­—ç¬¦
    
    for inst in text_instances:
        # è·å–å‘¨å›´æ›´å¤§çš„æ–‡æœ¬åŒºåŸŸ
        expanded_rect = fitz.Rect(
            inst.x0 - 50, inst.y0 - 20, 
            inst.x1 + 200, inst.y1 + 100
        )
        
        # ç¡®ä¿åœ¨é¡µé¢èŒƒå›´å†…
        page_rect = page.rect
        expanded_rect = expanded_rect & page_rect
        
        region_text = page.get_textbox(expanded_rect)
        similarity = _calculate_similarity(chunk_text, _clean_text(region_text))
        
        if similarity > 0.3:  # è¾ƒä½çš„é˜ˆå€¼
            matches.append({
                "page": page_num + 1,
                "bbox": [expanded_rect.x0, expanded_rect.y0, expanded_rect.x1, expanded_rect.y1],
                "type": "direct_text_match",
                "found_text": region_text[:150] + "..." if len(region_text) > 150 else region_text,
                "similarity": similarity,
                "match_type": "ç›´æ¥æ–‡æœ¬åŒ¹é…"
            })
    
    return matches


def _find_keyword_matches(page, keywords: List[str], chunk_text: str, page_num: int, threshold: float) -> List[Dict[str, Any]]:
    """åŸºäºå…³é”®è¯çš„åŒ¹é…"""
    matches = []
    
    if not keywords:
        return matches
    import pymupdf as fitz
    # æœç´¢ä¸»è¦å…³é”®è¯
    main_keyword = keywords[0] if keywords else ""
    if len(main_keyword) >= 3:
        keyword_instances = page.search_for(main_keyword)
        
        for inst in keyword_instances:
            # æ‰©å±•æœç´¢åŒºåŸŸ
            expanded_rect = fitz.Rect(
                inst.x0 - 30, inst.y0 - 30,
                inst.x1 + 150, inst.y1 + 80
            )
            
            page_rect = page.rect
            expanded_rect = expanded_rect & page_rect
            
            region_text = page.get_textbox(expanded_rect)
            similarity = _calculate_similarity(chunk_text, _clean_text(region_text))
            
            if similarity >= threshold:
                matches.append({
                    "page": page_num + 1,
                    "bbox": [expanded_rect.x0, expanded_rect.y0, expanded_rect.x1, expanded_rect.y1],
                    "type": "keyword_match",
                    "found_text": region_text[:150] + "..." if len(region_text) > 150 else region_text,
                    "similarity": similarity,
                    "match_type": "å…³é”®è¯åŒ¹é…",
                    "matched_keyword": main_keyword
                })
    
    return matches


def _find_fuzzy_matches(page, chunk_text: str, page_num: int, threshold: float) -> List[Dict[str, Any]]:
    """æ¨¡ç³ŠåŒ¹é… - é€æ®µè½æœç´¢"""
    matches = []
    
    # è·å–é¡µé¢æ‰€æœ‰æ–‡æœ¬å—
    text_dict = page.get_text("dict")
    text_blocks = []
    
    for block in text_dict["blocks"]:
        if block.get("type") == 0:  # æ–‡æœ¬å—
            block_text = ""
            for line in block["lines"]:
                for span in line["spans"]:
                    block_text += span["text"] + " "
            
            if block_text.strip():
                text_blocks.append({
                    "text": _clean_text(block_text),
                    "bbox": block["bbox"]
                })
    
    # é€å—æ£€æŸ¥ç›¸ä¼¼åº¦
    for block in text_blocks:
        similarity = _calculate_similarity(chunk_text, block["text"])
        
        if similarity >= threshold:
            matches.append({
                "page": page_num + 1,
                "bbox": block["bbox"],
                "type": "fuzzy_match",
                "found_text": block["text"][:150] + "..." if len(block["text"]) > 150 else block["text"],
                "similarity": similarity,
                "match_type": "æ¨¡ç³ŠåŒ¹é…"
            })
    
    return matches


def _process_short_text_results(results: List[Dict[str, Any]], return_best_only: bool) -> List[Dict[str, Any]]:
    """å¤„ç†çŸ­æ–‡æœ¬åŒ¹é…ç»“æœ"""
    if not results:
        return results
    
    # æŒ‰ç›¸ä¼¼åº¦æ’åº
    results.sort(key=lambda x: x['similarity'], reverse=True)
    
    # å»é‡
    results = _remove_duplicate_regions(results)
    
    # çŸ­æ–‡æœ¬ä¼˜å…ˆè¿”å›ç›¸ä¼¼åº¦æœ€é«˜çš„
    if return_best_only and results:
        return [results[0]]
    
    return results[:5]  # çŸ­æ–‡æœ¬æœ€å¤šè¿”å›5ä¸ªç»“æœ


def _clean_text(text: str) -> str:
    """æ¸…ç†å’Œæ ‡å‡†åŒ–æ–‡æœ¬"""
    # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
    text = re.sub(r'\s+', ' ', text)
    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ä½†ä¿ç•™åŸºæœ¬æ ‡ç‚¹
    text = re.sub(r'[^\w\s\u4e00-\u9fff.,!?;:()\[\]{}""\'\'-]', ' ', text)
    return text.strip()


def _split_into_sentences(text: str) -> List[str]:
    """å°†æ–‡æœ¬åˆ†å‰²æˆå¥å­"""
    # ä½¿ç”¨å¤šç§æ ‡ç‚¹ç¬¦å·åˆ†å‰²
    sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿï¼›;]\s*', text)
    # è¿‡æ»¤æ‰å¤ªçŸ­çš„å¥å­
    sentences = [s.strip() for s in sentences if len(s.strip()) > 10]
    return sentences


def _extract_page_content(page) -> Dict[str, Any]:
    """æå–é¡µé¢çš„æ‰€æœ‰å†…å®¹"""
    try:
        import pymupdf as fitz
    except ImportError:
        try:
            import fitz
        except ImportError:
            return {}
    
    # è·å–æ–‡æœ¬å—ä¿¡æ¯
    text_dict = page.get_text("dict")
    
    content = {
        "text_blocks": [],
        "images": [],
        "tables": [],
        "page_size": {"width": page.rect.width, "height": page.rect.height}
    }
    
    # å¤„ç†æ–‡æœ¬å—
    for block in text_dict["blocks"]:
        if block.get("type") == 0:  # æ–‡æœ¬å—
            block_text = ""
            block_bbox = block["bbox"]
            
            for line in block["lines"]:
                for span in line["spans"]:
                    block_text += span["text"] + " "
            
            if block_text.strip():
                content["text_blocks"].append({
                    "text": _clean_text(block_text),
                    "bbox": block_bbox,
                    "raw_text": block_text
                })
        
        elif block.get("type") == 1:  # å›¾åƒå—
            content["images"].append({
                "bbox": block["bbox"],
                "width": block.get("width", 0),
                "height": block.get("height", 0)
            })
    
    # æ£€æµ‹è¡¨æ ¼ï¼ˆç®€å•ç‰ˆæœ¬ï¼‰
    drawings = page.get_drawings()
    table_areas = _detect_table_areas(drawings)
    
    for table_area in table_areas:
        table_text = page.get_textbox(fitz.Rect(table_area))
        if table_text.strip():
            content["tables"].append({
                "bbox": table_area,
                "text": _clean_text(table_text),
                "raw_text": table_text
            })
    
    return content


def _detect_table_areas(drawings) -> List[List[float]]:
    """ç®€å•çš„è¡¨æ ¼æ£€æµ‹"""
    lines = []
    rects = []
    
    for drawing in drawings:
        if drawing.get("type") == "l":  # çº¿æ¡
            items = drawing.get("items", [])
            for item in items:
                if item[0] == "l":  # line
                    lines.append(item[1:])  # [x0, y0, x1, y1]
        elif drawing.get("type") == "re":  # çŸ©å½¢
            rect = drawing.get("rect", [])
            if len(rect) == 4:
                rects.append(rect)
    
    # å¦‚æœæœ‰è¶³å¤Ÿå¤šçš„çº¿æ¡æˆ–çŸ©å½¢ï¼Œè®¤ä¸ºå¯èƒ½æ˜¯è¡¨æ ¼
    if len(lines) > 5 or len(rects) > 3:
        all_coords = lines + rects
        if all_coords:
            min_x = min(coord[0] for coord in all_coords)
            min_y = min(coord[1] for coord in all_coords)
            max_x = max(coord[2] for coord in all_coords)
            max_y = max(coord[3] for coord in all_coords)
            
            return [[min_x, min_y, max_x, max_y]]
    
    return []


def _locate_best_region_in_page(page, chunk_text: str, keywords: List[str], 
                               page_num: int) -> Optional[Dict[str, Any]]:
    """åœ¨é¡µé¢ä¸­å®šä½æœ€ä½³åŒ¹é…åŒºåŸŸ"""
    
    # è·å–æ–‡æœ¬å—ä¿¡æ¯
    text_dict = page.get_text("dict")
    
    best_match = None
    best_similarity = 0
    best_bbox = None
    
    # ç»„åˆç›¸é‚»çš„æ–‡æœ¬å—æ¥å½¢æˆæ›´å¤§çš„æ–‡æœ¬åŒºåŸŸ
    text_regions = _group_text_blocks(text_dict["blocks"])
    
    for region in text_regions:
        region_text = _clean_text(region["text"])
        similarity = _calculate_similarity(chunk_text, region_text)
        
        # é¢å¤–è€ƒè™‘å…³é”®è¯åŒ¹é…åº¦
        keyword_score = _calculate_keyword_density(keywords, region_text)
        combined_score = similarity * 0.7 + keyword_score * 0.3
        
        if combined_score > best_similarity:
            best_similarity = combined_score
            best_match = region_text
            best_bbox = region["bbox"]
    
    if best_match and best_similarity > 0.3:
        return {
            "page": page_num + 1,
            "bbox": best_bbox,
            "type": "best_region_match",
            "found_text": best_match[:200] + "..." if len(best_match) > 200 else best_match,
            "similarity": best_similarity,
            "match_type": "æ•´ä½“åŒºåŸŸåŒ¹é…"
        }
    
    return None


def _group_text_blocks(blocks) -> List[Dict[str, Any]]:
    """å°†ç›¸é‚»çš„æ–‡æœ¬å—ç»„åˆæˆæ›´å¤§çš„æ–‡æœ¬åŒºåŸŸ"""
    text_blocks = []
    
    # æå–æ‰€æœ‰æ–‡æœ¬å—
    for block in blocks:
        if block.get("type") == 0:  # æ–‡æœ¬å—
            block_text = ""
            for line in block["lines"]:
                for span in line["spans"]:
                    block_text += span["text"] + " "
            
            if block_text.strip():
                text_blocks.append({
                    "text": block_text.strip(),
                    "bbox": block["bbox"]
                })
    
    if not text_blocks:
        return []
    
    # æŒ‰Yåæ ‡æ’åº
    text_blocks.sort(key=lambda x: x["bbox"][1])
    
    # ç»„åˆç›¸é‚»çš„æ–‡æœ¬å—
    regions = []
    current_region = {
        "text": text_blocks[0]["text"],
        "bbox": list(text_blocks[0]["bbox"])
    }
    
    for i in range(1, len(text_blocks)):
        prev_bbox = current_region["bbox"]
        curr_bbox = text_blocks[i]["bbox"]
        
        # å¦‚æœå‚ç›´è·ç¦»å°äº50ï¼Œè®¤ä¸ºæ˜¯åŒä¸€åŒºåŸŸ
        vertical_gap = curr_bbox[1] - prev_bbox[3]
        
        if vertical_gap < 50:
            # åˆå¹¶åˆ°å½“å‰åŒºåŸŸ
            current_region["text"] += " " + text_blocks[i]["text"]
            # æ‰©å±•è¾¹ç•Œæ¡†
            current_region["bbox"][0] = min(current_region["bbox"][0], curr_bbox[0])
            current_region["bbox"][1] = min(current_region["bbox"][1], curr_bbox[1])
            current_region["bbox"][2] = max(current_region["bbox"][2], curr_bbox[2])
            current_region["bbox"][3] = max(current_region["bbox"][3], curr_bbox[3])
        else:
            # ä¿å­˜å½“å‰åŒºåŸŸï¼Œå¼€å§‹æ–°åŒºåŸŸ
            regions.append(current_region)
            current_region = {
                "text": text_blocks[i]["text"],
                "bbox": list(curr_bbox)
            }
    
    # æ·»åŠ æœ€åä¸€ä¸ªåŒºåŸŸ
    regions.append(current_region)
    
    return regions


def _locate_keyword_region(page, keywords: List[str], page_num: int, 
                          chunk_text: str) -> Optional[Dict[str, Any]]:
    """åŸºäºå…³é”®è¯å¯†åº¦å®šä½åŒºåŸŸ"""
    
    text_dict = page.get_text("dict")
    text_regions = _group_text_blocks(text_dict["blocks"])
    
    best_region = None
    best_score = 0
    
    for region in text_regions:
        region_text = _clean_text(region["text"])
        keyword_density = _calculate_keyword_density(keywords, region_text)
        
        if keyword_density > best_score:
            best_score = keyword_density
            best_region = region
    
    if best_region and best_score > 0.2:
        # è®¡ç®—ä¸åŸæ–‡çš„ç›¸ä¼¼åº¦
        similarity = _calculate_similarity(chunk_text, _clean_text(best_region["text"]))
        
        return {
            "page": page_num + 1,
            "bbox": best_region["bbox"],
            "type": "keyword_region_match",
            "found_text": best_region["text"][:200] + "..." if len(best_region["text"]) > 200 else best_region["text"],
            "similarity": similarity,
            "keyword_density": best_score,
            "match_type": "å…³é”®è¯åŒºåŸŸåŒ¹é…"
        }
    
    return None


def _extract_key_phrases(text: str) -> List[str]:
    """æå–å…³é”®çŸ­è¯­å’Œè¯æ±‡"""
    # å»é™¤æ ‡ç‚¹ç¬¦å·å’Œæ•°å­—ï¼Œä¿ç•™æœ‰æ„ä¹‰çš„è¯æ±‡
    words = re.findall(r'[\u4e00-\u9fff]+|[a-zA-Z]{2,}', text)
    
    # è¿‡æ»¤æ‰è¿‡çŸ­çš„è¯å’Œå¸¸è§åœç”¨è¯
    stopwords = {'çš„', 'å’Œ', 'æˆ–', 'åŠ', 'ä¸', 'å¯¹', 'ä»', 'åœ¨', 'ä¸º', 'æ˜¯', 'äº†', 'åˆ°', 'ç”±', 'æœ‰', 'è¢«', 'æ‰€', 'ç­‰'}
    keywords = [word for word in words if len(word) >= 2 and word not in stopwords]
    
    # å»é‡å¹¶æŒ‰é•¿åº¦æ’åºï¼ˆä¼˜å…ˆé•¿è¯ï¼‰
    keywords = list(set(keywords))
    keywords.sort(key=len, reverse=True)
    
    return keywords[:20]  # è¿”å›å‰20ä¸ªå…³é”®è¯


def _calculate_keyword_density(keywords: List[str], text: str) -> float:
    """è®¡ç®—å…³é”®è¯åœ¨æ–‡æœ¬ä¸­çš„å¯†åº¦"""
    if not keywords or not text:
        return 0
    
    text_lower = text.lower()
    matched_keywords = 0
    
    for keyword in keywords:
        if keyword.lower() in text_lower:
            matched_keywords += 1
    
    return matched_keywords / len(keywords)


def _remove_duplicate_regions(results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """å»é™¤é‡å¤æˆ–é‡å çš„åŒºåŸŸ"""
    if len(results) <= 1:
        return results
    
    filtered = []
    
    for result in results:
        is_duplicate = False
        
        for existing in filtered:
            # æ£€æŸ¥æ˜¯å¦æ˜¯åŒä¸€é¡µçš„é‡å åŒºåŸŸ
            if result["page"] == existing["page"]:
                overlap = _calculate_bbox_overlap(result["bbox"], existing["bbox"])
                if overlap > 0.5:  # 50%ä»¥ä¸Šé‡å è®¤ä¸ºæ˜¯é‡å¤
                    is_duplicate = True
                    break
        
        if not is_duplicate:
            filtered.append(result)
    
    return filtered


def _calculate_similarity(text1: str, text2: str) -> float:
    """è®¡ç®—ä¸¤æ®µæ–‡æœ¬çš„ç›¸ä¼¼åº¦"""
    # ä½¿ç”¨åºåˆ—åŒ¹é…å™¨è®¡ç®—ç›¸ä¼¼åº¦
    matcher = SequenceMatcher(None, text1.lower(), text2.lower())
    similarity = matcher.ratio()
    
    # å¦‚æœä¸€ä¸ªæ–‡æœ¬åŒ…å«å¦ä¸€ä¸ªæ–‡æœ¬ï¼Œç»™äºˆé¢å¤–åŠ åˆ†
    if text1.lower() in text2.lower() or text2.lower() in text1.lower():
        similarity = max(similarity, 0.8)
    
    return similarity


def _calculate_bbox_overlap(bbox1: List[float], bbox2: List[float]) -> float:
    """è®¡ç®—ä¸¤ä¸ªè¾¹ç•Œæ¡†çš„é‡å æ¯”ä¾‹"""
    x1_min, y1_min, x1_max, y1_max = bbox1
    x2_min, y2_min, x2_max, y2_max = bbox2
    
    # è®¡ç®—äº¤é›†
    intersection_x_min = max(x1_min, x2_min)
    intersection_y_min = max(y1_min, y2_min)
    intersection_x_max = min(x1_max, x2_max)
    intersection_y_max = min(y1_max, y2_max)
    
    if intersection_x_min >= intersection_x_max or intersection_y_min >= intersection_y_max:
        return 0  # æ— äº¤é›†
    
    intersection_area = (intersection_x_max - intersection_x_min) * (intersection_y_max - intersection_y_min)
    
    # è®¡ç®—å¹¶é›†
    area1 = (x1_max - x1_min) * (y1_max - y1_min)
    area2 = (x2_max - x2_min) * (y2_max - y2_min)
    union_area = area1 + area2 - intersection_area
    
    return intersection_area / union_area if union_area > 0 else 0


def _clean_text(text: str) -> str:
    """æ¸…ç†å’Œæ ‡å‡†åŒ–æ–‡æœ¬"""
    if not text:
        return ""
    
    # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
    text = re.sub(r'\s+', ' ', text.strip())
    # ç§»é™¤ä¸€äº›ç‰¹æ®Šå­—ç¬¦ï¼Œä½†ä¿ç•™ä¸­è‹±æ–‡å’ŒåŸºæœ¬æ ‡ç‚¹
    text = re.sub(r'[^\w\s\u4e00-\u9fffï¼Œã€‚ï¼ï¼Ÿï¼›ï¼šã€]', ' ', text)
    # å†æ¬¡æ¸…ç†ç©ºç™½
    text = re.sub(r'\s+', ' ', text.strip())
    
    return text


def _split_into_sentences(text: str) -> List[str]:
    """å°†æ–‡æœ¬åˆ†å‰²æˆå¥å­"""
    # æŒ‰æ ‡ç‚¹ç¬¦å·åˆ†å‰²
    sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\.\!\?]+', text)
    # è¿‡æ»¤ç©ºå¥å­
    sentences = [s.strip() for s in sentences if s.strip()]
    return sentences


def _calculate_similarity(text1: str, text2: str) -> float:
    """è®¡ç®—ä¸¤æ®µæ–‡æœ¬çš„ç›¸ä¼¼åº¦"""
    if not text1 or not text2:
        return 0.0
    
    # ä½¿ç”¨åºåˆ—åŒ¹é…å™¨è®¡ç®—ç›¸ä¼¼åº¦
    matcher = SequenceMatcher(None, text1.lower(), text2.lower())
    return matcher.ratio()


def analyze_chunk_content(chunk_text: str) -> Dict[str, Any]:
    """åˆ†æçŸ¥è¯†åˆ‡ç‰‡çš„å†…å®¹ç‰¹å¾"""
    analysis = {
        "length": len(chunk_text),
        "sentences": len(_split_into_sentences(chunk_text)),
        "has_numbers": bool(re.search(r'\d+', chunk_text)),
        "has_punctuation": bool(re.search(r'[.!?ã€‚ï¼ï¼Ÿ]', chunk_text)),
        "language_detected": "mixed",  # ç®€åŒ–ç‰ˆæœ¬
        "complexity_score": 0.0
    }
    
    # è®¡ç®—å¤æ‚åº¦åˆ†æ•°
    complexity_factors = [
        len(chunk_text) / 1000,  # é•¿åº¦å› å­
        analysis["sentences"] / 10,  # å¥å­æ•°é‡å› å­
        len(re.findall(r'[,;:]', chunk_text)) / 20,  # æ ‡ç‚¹å¤æ‚åº¦
    ]
    
    analysis["complexity_score"] = min(sum(complexity_factors), 1.0)
    
    return analysis


# æ¼”ç¤ºå’Œæµ‹è¯•å‡½æ•°
def demo_rag_locator():
    """æ¼”ç¤ºRAGåˆ‡ç‰‡å®šä½åŠŸèƒ½"""
    try:
        import pymupdf as fitz
    except ImportError:
        print("éœ€è¦å®‰è£…PyMuPDFæ¥è¿è¡Œæ¼”ç¤º")
        return
    
    # åˆ›å»ºæµ‹è¯•PDF
    doc = fitz.open()
    page = doc.new_page()
    
    # æ·»åŠ å¤æ‚å†…å®¹æ¨¡æ‹ŸRAGåˆ‡ç‰‡åœºæ™¯
    content_blocks = [
        "äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligenceï¼ŒAIï¼‰æ˜¯æŒ‡ç”±äººåˆ¶é€ å‡ºæ¥çš„æœºå™¨æ‰€è¡¨ç°å‡ºæ¥çš„æ™ºèƒ½ã€‚",
        "é€šå¸¸äººå·¥æ™ºèƒ½æ˜¯æŒ‡é€šè¿‡æ™®é€šè®¡ç®—æœºç¨‹åºæ¥å‘ˆç°äººç±»æ™ºèƒ½çš„æŠ€æœ¯ã€‚",
        "",  # ç©ºè¡Œ
        "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜ç¡®ç¼–ç¨‹çš„æƒ…å†µä¸‹å­¦ä¹ ã€‚",
        "æ·±åº¦å­¦ä¹ åˆ™æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é›†ï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥å¤„ç†å¤æ‚çš„æ•°æ®æ¨¡å¼ã€‚",
    ]
    
    y_pos = 100
    for block in content_blocks:
        if block:  # éç©ºè¡Œ
            page.insert_text((50, y_pos), block, fontsize=12)
        y_pos += 25
    
    # æ·»åŠ è¡¨æ ¼
    table_y = 250
    page.draw_rect(fitz.Rect(50, table_y, 450, table_y + 100), width=1)
    
    # è¡¨æ ¼å†…å®¹
    headers = ["æŠ€æœ¯", "åº”ç”¨é¢†åŸŸ", "å‘å±•é˜¶æ®µ"]
    data = [
        ["æœºå™¨å­¦ä¹ ", "æ•°æ®åˆ†æ", "æˆç†Ÿ"],
        ["æ·±åº¦å­¦ä¹ ", "å›¾åƒè¯†åˆ«", "å¿«é€Ÿå‘å±•"],
    ]
    
    # ç»˜åˆ¶è¡¨æ ¼
    for i, header in enumerate(headers):
        page.insert_text((70 + i * 120, table_y + 20), header, fontsize=10)
    
    for i, row in enumerate(data):
        for j, cell in enumerate(row):
            page.insert_text((70 + j * 120, table_y + 40 + i * 25), cell, fontsize=10)
    
    test_pdf = "rag_test.pdf"
    doc.save(test_pdf)
    doc.close()
    print(f"RAGæµ‹è¯•PDFå·²åˆ›å»º: {test_pdf}")
    
    # æ¨¡æ‹ŸRAGçŸ¥è¯†åˆ‡ç‰‡
    rag_chunks = [
        # åˆ‡ç‰‡1ï¼šå•æ®µæ–‡æœ¬
        "äººå·¥æ™ºèƒ½æ˜¯æŒ‡ç”±äººåˆ¶é€ å‡ºæ¥çš„æœºå™¨æ‰€è¡¨ç°å‡ºæ¥çš„æ™ºèƒ½ã€‚é€šå¸¸äººå·¥æ™ºèƒ½æ˜¯æŒ‡é€šè¿‡æ™®é€šè®¡ç®—æœºç¨‹åºæ¥å‘ˆç°äººç±»æ™ºèƒ½çš„æŠ€æœ¯ã€‚",
        
        # åˆ‡ç‰‡2ï¼šå¤šæ®µæ–‡æœ¬
        "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜ç¡®ç¼–ç¨‹çš„æƒ…å†µä¸‹å­¦ä¹ ã€‚æ·±åº¦å­¦ä¹ åˆ™æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é›†ï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥å¤„ç†å¤æ‚çš„æ•°æ®æ¨¡å¼ã€‚",
        
        # åˆ‡ç‰‡3ï¼šéƒ¨åˆ†æ–‡æœ¬
        "æ·±åº¦å­¦ä¹ åˆ™æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é›†",
        
        # åˆ‡ç‰‡4ï¼šåŒ…å«è¡¨æ ¼ä¿¡æ¯çš„æ–‡æœ¬
        "æœºå™¨å­¦ä¹ åœ¨æ•°æ®åˆ†æé¢†åŸŸåº”ç”¨å¹¿æ³›ï¼Œç›®å‰å‘å±•å·²ç»ç›¸å¯¹æˆç†Ÿã€‚"
    ]
    
    print("\nå¼€å§‹å®šä½RAGçŸ¥è¯†åˆ‡ç‰‡...")
    
    for i, chunk in enumerate(rag_chunks, 1):
        print(f"\n--- åˆ‡ç‰‡ {i} ---")
        print(f"å†…å®¹: {chunk[:100]}...")
        
        # åˆ†æåˆ‡ç‰‡ç‰¹å¾
        analysis = analyze_chunk_content(chunk)
        print(f"åˆ†æ: é•¿åº¦={analysis['length']}, å¥å­æ•°={analysis['sentences']}, å¤æ‚åº¦={analysis['complexity_score']:.2f}")
        
        # å®šä½åˆ‡ç‰‡
        results = find_rag_chunk_coordinates(chunk, test_pdf, similarity_threshold=0.4)
        
        if results:
            print(f"æ‰¾åˆ° {len(results)} ä¸ªåŒ¹é…ä½ç½®:")
            for j, result in enumerate(results, 1):
                print(f"  {j}. é¡µç : {result['page']}")
                print(f"     åæ ‡: {result['bbox']}")
                print(f"     ç±»å‹: {result['type']}")
                print(f"     ç›¸ä¼¼åº¦: {result['similarity']:.3f}")
                
                if result['type'] == 'combined_match':
                    print(f"     è¦†ç›–åŒºåŸŸ: {result['coverage_area']['width']:.1f} x {result['coverage_area']['height']:.1f}")
                    print(f"     åŒ…å«éƒ¨åˆ†: {result['chunk_parts']}")
        else:
            print("æœªæ‰¾åˆ°åŒ¹é…ä½ç½®")


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) >= 3:
        chunk_content = sys.argv[1]
        pdf_file = sys.argv[2]
        
        # å¯é€‰å‚æ•°
        similarity_threshold = 0.6
        if len(sys.argv) > 3:
            try:
                similarity_threshold = float(sys.argv[3])
            except ValueError:
                print("ç›¸ä¼¼åº¦é˜ˆå€¼å¿…é¡»æ˜¯0-1ä¹‹é—´çš„æ•°å­—")
                sys.exit(1)
        
        print(f"åœ¨PDF '{pdf_file}' ä¸­å®šä½çŸ¥è¯†åˆ‡ç‰‡")
        print(f"ç›¸ä¼¼åº¦é˜ˆå€¼: {similarity_threshold}")
        print(f"åˆ‡ç‰‡å†…å®¹: {chunk_content[:100]}...")
        
        # åˆ†æåˆ‡ç‰‡
        analysis = analyze_chunk_content(chunk_content)
        print(f"\nåˆ‡ç‰‡åˆ†æ:")
        print(f"- é•¿åº¦: {analysis['length']} å­—ç¬¦")
        print(f"- å¥å­æ•°: {analysis['sentences']}")
        print(f"- å¤æ‚åº¦: {analysis['complexity_score']:.2f}")
        
        # å®šä½åˆ‡ç‰‡
        results = find_rag_chunk_coordinates(chunk_content, pdf_file, similarity_threshold)
        
        if results:
            print(f"\næ‰¾åˆ° {len(results)} ä¸ªåŒ¹é…ä½ç½®:")
            for i, result in enumerate(results, 1):
                print(f"\n{i}. é¡µç : {result['page']}")
                print(f"   åæ ‡: {result['bbox']}")
                print(f"   ç±»å‹: {result['type']}")
                print(f"   ç›¸ä¼¼åº¦: {result['similarity']:.3f}")
                
                if result['type'] == 'combined_match':
                    print(f"   è¦†ç›–åŒºåŸŸ: {result['coverage_area']['width']:.1f} x {result['coverage_area']['height']:.1f}")
                    print(f"   åŒ…å«éƒ¨åˆ†: {result['chunk_parts']}")
                    print(f"   åŒ¹é…å¥å­æ•°: {len(result['matched_sentences'])}")
                
                # ä¿å­˜è¯¦ç»†ç»“æœåˆ°JSON
                output_file = f"rag_chunk_location_{i}.json"
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(result, f, ensure_ascii=False, indent=2)
                print(f"   è¯¦ç»†ç»“æœå·²ä¿å­˜: {output_file}")
        else:
            print("æœªæ‰¾åˆ°åŒ¹é…çš„ä½ç½®")
            print("å»ºè®®:")
            print("- é™ä½ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆå½“å‰: {:.2f}ï¼‰".format(similarity_threshold))
            print("- æ£€æŸ¥åˆ‡ç‰‡å†…å®¹æ˜¯å¦ç¡®å®åœ¨PDFä¸­")
            print("- åˆ‡ç‰‡å¯èƒ½è·¨é¡µæˆ–æ ¼å¼å·®å¼‚è¾ƒå¤§")
    
    else:
        print("RAGçŸ¥è¯†åˆ‡ç‰‡åæ ‡å®šä½å™¨æ¼”ç¤º")
        print("=" * 50)
        demo_rag_locator()
        print("\n" + "=" * 50)
        print("ç”¨æ³•:")
        print("python rag_chunk_locator.py 'çŸ¥è¯†åˆ‡ç‰‡å†…å®¹' 'pdfæ–‡ä»¶è·¯å¾„' [ç›¸ä¼¼åº¦é˜ˆå€¼]")
        print("ç›¸ä¼¼åº¦é˜ˆå€¼èŒƒå›´: 0.0-1.0ï¼Œé»˜è®¤0.6ï¼Œè¶Šä½åŒ¹é…è¶Šå®½æ¾") 