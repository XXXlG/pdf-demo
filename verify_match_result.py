#!/usr/bin/env python3
"""
RAGåŒ¹é…ç»“æœéªŒè¯å·¥å…·
ç”¨äºéªŒè¯å’Œå¯è§†åŒ–RAGåˆ‡ç‰‡åœ¨PDFä¸­çš„åŒ¹é…ç»“æœ
"""

import re
from typing import Dict, Any, List
from difflib import SequenceMatcher

def verify_match_result(original_chunk: str, pdf_path: str, match_result: Dict[str, Any], 
                       create_visual: bool = True) -> Dict[str, Any]:
    """
    éªŒè¯åŒ¹é…ç»“æœçš„å‡†ç¡®æ€§
    
    Args:
        original_chunk: åŸå§‹åˆ‡ç‰‡æ–‡æœ¬
        pdf_path: PDFæ–‡ä»¶è·¯å¾„
        match_result: åŒ¹é…ç»“æœå­—å…¸
        create_visual: æ˜¯å¦åˆ›å»ºå¯è§†åŒ–éªŒè¯
    
    Returns:
        éªŒè¯æŠ¥å‘Š
    """
    try:
        import pymupdf as fitz
    except ImportError:
        try:
            import fitz
        except ImportError:
            raise ImportError("è¯·å®‰è£…PyMuPDF: pip install pymupdf")
    
    print("ğŸ” å¼€å§‹éªŒè¯åŒ¹é…ç»“æœ...")
    print("=" * 50)
    
    # 1. åŸºæœ¬ä¿¡æ¯éªŒè¯
    page_num = match_result["page"] - 1  # è½¬æ¢ä¸º0ç´¢å¼•
    bbox = match_result["bbox"]
    
    print(f"ğŸ“„ ç›®æ ‡é¡µç : {match_result['page']}")
    print(f"ğŸ“ åŒ¹é…åæ ‡: {bbox}")
    print(f"ğŸ“Š æŠ¥å‘Šç›¸ä¼¼åº¦: {match_result['similarity']:.3f}")
    
    # 2. æå–å®é™…åŒ¹é…åŒºåŸŸçš„å®Œæ•´æ–‡æœ¬
    try:
        import pymupdf as fitz
    except ImportError:
        try:
            import fitz
        except ImportError:
            raise ImportError("è¯·å®‰è£…PyMuPDF: pip install pymupdf")
    
    doc = fitz.open(pdf_path)
    
    if page_num >= doc.page_count:
        return {"error": f"é¡µç è¶…å‡ºèŒƒå›´ï¼ŒPDFåªæœ‰{doc.page_count}é¡µ"}
    
    page = doc[page_num]
    
    # æå–åŒ¹é…åŒºåŸŸçš„æ–‡æœ¬
    match_rect = fitz.Rect(bbox)
    extracted_text = page.get_textbox(match_rect)
    extracted_text_clean = _clean_text_for_comparison(extracted_text)
    
    print(f"\nğŸ“ æå–çš„å®Œæ•´åŒ¹é…æ–‡æœ¬:")
    print(f"åŸå§‹é•¿åº¦: {len(extracted_text)} å­—ç¬¦")
    print(f"æ¸…ç†åé•¿åº¦: {len(extracted_text_clean)} å­—ç¬¦")
    print("-" * 30)
    print(extracted_text[:300] + "..." if len(extracted_text) > 300 else extracted_text)
    print("-" * 30)
    
    # 3. é‡æ–°è®¡ç®—ç›¸ä¼¼åº¦
    original_clean = _clean_text_for_comparison(original_chunk)
    actual_similarity = _calculate_detailed_similarity(original_clean, extracted_text_clean)
    
    print(f"\nğŸ“Š è¯¦ç»†ç›¸ä¼¼åº¦åˆ†æ:")
    print(f"æŠ¥å‘Šç›¸ä¼¼åº¦: {match_result['similarity']:.3f}")
    print(f"å®é™…ç›¸ä¼¼åº¦: {actual_similarity['overall']:.3f}")
    print(f"å­—ç¬¦çº§åŒ¹é…: {actual_similarity['char_level']:.3f}")
    print(f"è¯æ±‡çº§åŒ¹é…: {actual_similarity['word_level']:.3f}")
    print(f"å…³é”®è¯è¦†ç›–: {actual_similarity['keyword_coverage']:.3f}")
    
    # 4. æ–‡æœ¬å¯¹æ¯”åˆ†æ
    comparison = _analyze_text_differences(original_clean, extracted_text_clean)
    print(f"\nğŸ” æ–‡æœ¬å¯¹æ¯”åˆ†æ:")
    print(f"å…±åŒè¯æ±‡: {len(comparison['common_words'])}")
    print(f"ç¼ºå¤±è¯æ±‡: {len(comparison['missing_words'])}")
    print(f"å¤šä½™è¯æ±‡: {len(comparison['extra_words'])}")
    
    if comparison['missing_words']:
        print(f"ä¸»è¦ç¼ºå¤±è¯æ±‡: {comparison['missing_words'][:5]}")
    
    # 5. åˆ›å»ºå¯è§†åŒ–éªŒè¯
    verification_report = {
        "page": match_result["page"],
        "bbox": bbox,
        "reported_similarity": match_result["similarity"],
        "actual_similarity": actual_similarity,
        "text_comparison": comparison,
        "extracted_text": extracted_text[:500],  # å‰500å­—ç¬¦
        "verification_status": "PASS" if actual_similarity["overall"] > 0.3 else "FAIL"
    }
    
    if create_visual:
        visual_path = _create_visual_verification(doc, page_num, match_rect, pdf_path)
        verification_report["visual_verification"] = visual_path
        print(f"\nğŸ–¼ï¸  å¯è§†åŒ–éªŒè¯å·²ä¿å­˜: {visual_path}")
    
    doc.close()
    
    # 6. ç”ŸæˆéªŒè¯ç»“è®º
    print(f"\nâœ… éªŒè¯ç»“è®º:")
    if verification_report["verification_status"] == "PASS":
        print(f"   âœ“ åŒ¹é…ç»“æœæœ‰æ•ˆ (ç›¸ä¼¼åº¦: {actual_similarity['overall']:.3f})")
        print(f"   âœ“ æ‰¾åˆ°äº†ç›¸å…³å†…å®¹åŒºåŸŸ")
    else:
        print(f"   âŒ åŒ¹é…ç»“æœå¯ç–‘ (ç›¸ä¼¼åº¦: {actual_similarity['overall']:.3f})")
        print(f"   âŒ å¯èƒ½éœ€è¦è°ƒæ•´åŒ¹é…å‚æ•°")
    
    return verification_report


def _clean_text_for_comparison(text: str) -> str:
    """ä¸ºå¯¹æ¯”åˆ†ææ¸…ç†æ–‡æœ¬"""
    if not text:
        return ""
    
    # ç§»é™¤å¤šä½™ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦
    text = re.sub(r'\s+', ' ', text.strip())
    # ç§»é™¤å›¾ç‰‡å¼•ç”¨ç­‰
    text = re.sub(r'!\[.*?\]\(.*?\)', '', text)
    text = re.sub(r'http[s]?://\S+', '', text)
    # æ ‡å‡†åŒ–æ ‡ç‚¹
    text = re.sub(r'[ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼šã€]', ' ', text)
    text = re.sub(r'\s+', ' ', text.strip())
    
    return text


def _calculate_detailed_similarity(text1: str, text2: str) -> Dict[str, float]:
    """è®¡ç®—è¯¦ç»†çš„ç›¸ä¼¼åº¦æŒ‡æ ‡"""
    if not text1 or not text2:
        return {"overall": 0.0, "char_level": 0.0, "word_level": 0.0, "keyword_coverage": 0.0}
    
    # å­—ç¬¦çº§ç›¸ä¼¼åº¦
    char_similarity = SequenceMatcher(None, text1.lower(), text2.lower()).ratio()
    
    # è¯æ±‡çº§ç›¸ä¼¼åº¦
    words1 = set(re.findall(r'[\u4e00-\u9fff]+|[a-zA-Z]{2,}', text1.lower()))
    words2 = set(re.findall(r'[\u4e00-\u9fff]+|[a-zA-Z]{2,}', text2.lower()))
    
    if words1 and words2:
        word_similarity = len(words1 & words2) / len(words1 | words2)
    else:
        word_similarity = 0.0
    
    # å…³é”®è¯è¦†ç›–ç‡
    keywords1 = [w for w in words1 if len(w) >= 3]
    keywords2 = [w for w in words2 if len(w) >= 3]
    
    if keywords1:
        keyword_coverage = len(set(keywords1) & set(keywords2)) / len(keywords1)
    else:
        keyword_coverage = 0.0
    
    # ç»¼åˆç›¸ä¼¼åº¦
    overall = (char_similarity * 0.3 + word_similarity * 0.4 + keyword_coverage * 0.3)
    
    return {
        "overall": overall,
        "char_level": char_similarity,
        "word_level": word_similarity,
        "keyword_coverage": keyword_coverage
    }


def _analyze_text_differences(text1: str, text2: str) -> Dict[str, List[str]]:
    """åˆ†æä¸¤æ®µæ–‡æœ¬çš„å·®å¼‚"""
    words1 = set(re.findall(r'[\u4e00-\u9fff]+|[a-zA-Z]{2,}', text1.lower()))
    words2 = set(re.findall(r'[\u4e00-\u9fff]+|[a-zA-Z]{2,}', text2.lower()))
    
    return {
        "common_words": list(words1 & words2),
        "missing_words": list(words1 - words2),
        "extra_words": list(words2 - words1)
    }


def _create_visual_verification(doc, page_num: int, match_rect, pdf_path: str) -> str:
    """åˆ›å»ºå¯è§†åŒ–éªŒè¯å›¾ç‰‡"""
    try:
        import pymupdf as fitz
    except ImportError:
        try:
            import fitz
        except ImportError:
            print("æ— æ³•å¯¼å…¥PyMuPDFï¼Œè·³è¿‡å¯è§†åŒ–éªŒè¯")
            return None
    
    try:
        page = doc[page_num]
        
        # åœ¨åŒ¹é…åŒºåŸŸç»˜åˆ¶çº¢è‰²è¾¹æ¡†
        page.draw_rect(match_rect, color=(1, 0, 0), width=2)
        
        # æ·»åŠ æ ‡æ³¨
        text_point = fitz.Point(match_rect.x0, match_rect.y0 - 10)
        page.insert_text(text_point, f"åŒ¹é…åŒºåŸŸ {match_rect.width:.0f}x{match_rect.height:.0f}", 
                        fontsize=10, color=(1, 0, 0))
        
        # ä¿å­˜ä¸ºå›¾ç‰‡
        output_path = pdf_path.replace('.pdf', f'_verification_page_{page_num + 1}.png')
        pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))  # 2xç¼©æ”¾æé«˜æ¸…æ™°åº¦
        pix.save(output_path)
        
        return output_path
        
    except Exception as e:
        print(f"åˆ›å»ºå¯è§†åŒ–éªŒè¯å¤±è´¥: {e}")
        return None


def quick_verify():
    """å¿«é€ŸéªŒè¯æœ€è¿‘çš„åŒ¹é…ç»“æœ"""
    
    # ä½¿ç”¨ç¤ºä¾‹æ•°æ®
    from rag_chunk_locator import find_rag_chunk_coordinates
    
    knowledge_chunk = r"""
1ï¼‰å…ƒå™¨ä»¶å®‰è£…å­”ä¸å…ƒå™¨ä»¶å¼•çº¿ä¸åŒ¹é…# é—®é¢˜æè¿°å°åˆ¶æ¿å…ƒå™¨ä»¶å®‰è£…å­”å­”å¾„ä¸å…ƒå™¨ä»¶å¼•çº¿ç›´å¾„ä¸åŒ¹é…ï¼Œå¯¼è‡´å…ƒå™¨ä»¶å¼•çº¿æ— æ³•å®‰è£…æˆ–é—´éš™è¿‡å°ï¼Œå®¹æ˜“å¯¼è‡´å®‰è£…å­”æŸä¼¤æˆ–é€é”¡ä¸è‰¯ï¼Œå½±å“ç„Šç‚¹çš„å¯é æ€§ï¼Œè§å›¾1-1æ‰€ç¤ºã€‚
    """
    
    pdf_file = "data/èˆªå¤©ç”µå­äº§å“å¸¸è§è´¨é‡ç¼ºé™·æ¡ˆä¾‹.13610530(2).pdf"
    
    print("ğŸš€ æ‰§è¡Œå¿«é€ŸéªŒè¯...")
    
    try:
        # è·å–åŒ¹é…ç»“æœ
        results = find_rag_chunk_coordinates(knowledge_chunk, pdf_file, return_best_only=True)
        
        if results:
            result = results[0]
            
            # éªŒè¯ç»“æœ
            verification = verify_match_result(knowledge_chunk, pdf_file, result, create_visual=True)
            
            print(f"\nğŸ“‹ éªŒè¯æ‘˜è¦:")
            print(f"   çŠ¶æ€: {verification['verification_status']}")
            print(f"   å®é™…ç›¸ä¼¼åº¦: {verification['actual_similarity']['overall']:.3f}")
            print(f"   å…³é”®è¯è¦†ç›–: {verification['actual_similarity']['keyword_coverage']:.3f}")
            
            return verification
        else:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°åŒ¹é…ç»“æœ")
            return None
            
    except Exception as e:
        print(f"âŒ éªŒè¯å¤±è´¥: {e}")
        return None


if __name__ == "__main__":
    # è¿è¡Œå¿«é€ŸéªŒè¯
    quick_verify() 